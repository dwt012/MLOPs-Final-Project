{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc5707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from pyflink.common import WatermarkStrategy\n",
    "from pyflink.common.serialization import SimpleStringSchema\n",
    "from pyflink.common.typeinfo import Types\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.datastream.connectors.kafka import (\n",
    "    KafkaOffsetsInitializer, KafkaRecordSerializationSchema, KafkaSink,\n",
    "    KafkaSource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c38d6e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_features(record):\n",
    "    \"\"\"\n",
    "    Merged feature columns into one single data column\n",
    "    and keep other columns unchanged.\n",
    "    \"\"\"\n",
    "    # Convert Row to dict\n",
    "    record = json.loads(record)\n",
    "    data = record[\"payload\"][\"after\"]\n",
    "    print(data)\n",
    "    return json.dumps(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "486c60c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_features(record):\n",
    "    \"\"\"\n",
    "    Remove unnecessary columns\n",
    "    \"\"\"\n",
    "    record = json.loads(record)\n",
    "    data = {}\n",
    "    for key in record:\n",
    "        if key != \"created\" and key != \"content\":\n",
    "            data[key] = record[key]\n",
    "\n",
    "    return json.dumps({\"created\": record[\"created\"], \"data\": data})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b504899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_record_keys(record):\n",
    "    \"\"\"\n",
    "    Check messages have \"payload\" key or not\n",
    "    \"\"\"\n",
    "    # Convert Row to dict\n",
    "    record = json.loads(record)\n",
    "    keys = list(record.keys())\n",
    "    if \"payload\" in keys:\n",
    "        return True\n",
    "\n",
    "    print(\"record: \", list(record.keys()))\n",
    "    return True\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64a61729",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = StreamExecutionEnvironment.get_execution_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b717a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lại jar files path\n",
    "JARS_PATH = \"/Users/dangthiphuongthao/Documents/Apps/jars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "354481d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The other commented lines are for Avro format\n",
    "env.add_jars(\n",
    "    f\"file://{JARS_PATH}/flink-connector-kafka-4.0.0-2.0.jar\",\n",
    "    # f\"file://{JARS_PATH}/flink-avro-1.17.1.jar\",\n",
    "    # f\"file://{JARS_PATH}/flink-avro-confluent-registry-1.17.1.jar\",\n",
    "    # f\"file://{JARS_PATH}/avro-1.11.1.jar\",\n",
    "    # f\"file://{JARS_PATH}/jackson-databind-2.14.2.jar\",\n",
    "    # f\"file://{JARS_PATH}/jackson-core-2.14.2.jar\",\n",
    "    # f\"file://{JARS_PATH}/jackson-annotations-2.14.2.jar\",\n",
    "    f\"file://{JARS_PATH}/kafka-clients-3.6.0.jar\",\n",
    "    # f\"file://{JARS_PATH}/kafka-schema-registry-client-5.3.0.jar\",\n",
    ")\n",
    "\n",
    "# Avro will need it for validation from the schema registry\n",
    "# schema_path = \"./data_ingestion/kafka_producer/avro_schemas/schema_0.avsc\"\n",
    "# with open(schema_path) as f:\n",
    "#     schema = f.read()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7794300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyflink.common import Configuration\n",
    "\n",
    "config = Configuration()\n",
    "config.set_string(\n",
    "    \"pipeline.jars\",\n",
    "    \";\".join([\n",
    "        f\"file://{JARS_PATH}/flink-connector-kafka-4.0.0-2.0.jar\",\n",
    "        f\"file://{JARS_PATH}/kafka-clients-3.6.0.jar\"\n",
    "        # add thêm nếu cần nữa\n",
    "    ])\n",
    ")\n",
    "env = StreamExecutionEnvironment.get_execution_environment(configuration=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0449514a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///Users/dangthiphuongthao/Documents/Apps/jars/flink-connector-kafka-4.0.0-2.0.jar\n"
     ]
    }
   ],
   "source": [
    "print(f\"file://{JARS_PATH}/flink-connector-kafka-4.0.0-2.0.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "330dae47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the source to take data from\n",
    "source = (\n",
    "    KafkaSource.builder()\n",
    "    .set_bootstrap_servers(\"localhost:9092\")\n",
    "    .set_topics(\"diabetes_cdc.public.diabetes_new\")\n",
    "    .set_group_id(\"diabetes-consumer-group\")\n",
    "    .set_starting_offsets(KafkaOffsetsInitializer.latest())\n",
    "    .set_value_only_deserializer(SimpleStringSchema())\n",
    "    .build()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ed93541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sink to save the processed data to\n",
    "sink = (\n",
    "    KafkaSink.builder()\n",
    "    .set_bootstrap_servers(\"http://localhost:9092\")\n",
    "    .set_record_serializer(\n",
    "        KafkaRecordSerializationSchema.builder()\n",
    "        .set_topic(\"diabetes_out.public.sink_diabetes\")\n",
    "        .set_value_serialization_schema(SimpleStringSchema())\n",
    "        .build()\n",
    "    )\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# No sink, just print out to the terminal\n",
    "# env.from_source(source, WatermarkStrategy.no_watermarks(), \"Kafka Source\").filter(\n",
    "#     filter_small_features\n",
    "# ).map(merge_features).print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c16dbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-10T09:58:59.181949Z Source Data Fetcher for Source: Kafka Source -> Filter, Map, Map -> Sink: Writer -> Sink: Committer (8/8)#0 ERROR Unable to write to stream /opt/anaconda3/envs/flink-env/lib/python3.10/site-packages/pyflink/log/flink-nguyenthiphuongthao-python-Thaos-mac.lan.log for appender FileAppender org.apache.logging.log4j.core.appender.AppenderLoggingException: Error writing to stream /opt/anaconda3/envs/flink-env/lib/python3.10/site-packages/pyflink/log/flink-nguyenthiphuongthao-python-Thaos-mac.lan.log\n",
      "\tat org.apache.logging.log4j.core.appender.OutputStreamManager.writeToDestination(OutputStreamManager.java:265)\n",
      "\tat org.apache.logging.log4j.core.appender.FileManager.writeToDestination(FileManager.java:335)\n",
      "\tat org.apache.logging.log4j.core.appender.OutputStreamManager.flushBuffer(OutputStreamManager.java:296)\n",
      "\tat org.apache.logging.log4j.core.appender.OutputStreamManager.flush(OutputStreamManager.java:307)\n",
      "\tat org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.directEncodeEvent(AbstractOutputStreamAppender.java:229)\n",
      "\tat org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.tryAppend(AbstractOutputStreamAppender.java:220)\n",
      "\tat org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.append(AbstractOutputStreamAppender.java:211)\n",
      "\tat org.apache.logging.log4j.core.config.AppenderControl.tryCallAppender(AppenderControl.java:160)\n",
      "\tat org.apache.logging.log4j.core.config.AppenderControl.callAppender0(AppenderControl.java:133)\n",
      "\tat org.apache.logging.log4j.core.config.AppenderControl.callAppenderPreventRecursion(AppenderControl.java:124)\n",
      "\tat org.apache.logging.log4j.core.config.AppenderControl.callAppender(AppenderControl.java:88)\n",
      "\tat org.apache.logging.log4j.core.config.LoggerConfig.callAppenders(LoggerConfig.java:714)\n",
      "\tat org.apache.logging.log4j.core.config.LoggerConfig.processLogEvent(LoggerConfig.java:672)\n",
      "\tat org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:648)\n",
      "\tat org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:549)\n",
      "\tat org.apache.logging.log4j.core.config.AwaitCompletionReliabilityStrategy.log(AwaitCompletionReliabilityStrategy.java:67)\n",
      "\tat org.apache.logging.log4j.core.Logger.logMessage(Logger.java:226)\n",
      "\tat org.apache.logging.slf4j.Log4jLogger.log(Log4jLogger.java:374)\n",
      "\tat org.apache.kafka.common.utils.LogContext$LocationAwareKafkaLogger.writeLog(LogContext.java:434)\n",
      "\tat org.apache.kafka.common.utils.LogContext$LocationAwareKafkaLogger.info(LogContext.java:392)\n",
      "\tat org.apache.kafka.clients.FetchSessionHandler.handleResponse(FetchSessionHandler.java:528)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractFetch.handleFetchResponse(AbstractFetch.java:145)\n",
      "\tat org.apache.kafka.clients.consumer.internals.Fetcher$1.onSuccess(Fetcher.java:82)\n",
      "\tat org.apache.kafka.clients.consumer.internals.Fetcher$1.onSuccess(Fetcher.java:78)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:169)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:129)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:617)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:427)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:312)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:251)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1255)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1186)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1159)\n",
      "\tat org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.fetch(KafkaPartitionSplitReader.java:110)\n",
      "\tat org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)\n",
      "\tat org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:193)\n",
      "\tat org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:130)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:349)\n",
      "\tat org.apache.logging.log4j.core.appender.OutputStreamManager.writeToDestination(OutputStreamManager.java:263)\n",
      "\t... 41 more\n",
      "\n",
      "2025-05-10T09:58:59.197588Z Source Data Fetcher for Source: Kafka Source -> Filter, Map, Map -> Sink: Writer -> Sink: Committer (8/8)#0 ERROR An exception occurred processing Appender FileAppender org.apache.logging.log4j.core.appender.AppenderLoggingException: Error writing to stream /opt/anaconda3/envs/flink-env/lib/python3.10/site-packages/pyflink/log/flink-nguyenthiphuongthao-python-Thaos-mac.lan.log\n",
      "\tat org.apache.logging.log4j.core.appender.OutputStreamManager.writeToDestination(OutputStreamManager.java:265)\n",
      "\tat org.apache.logging.log4j.core.appender.FileManager.writeToDestination(FileManager.java:335)\n",
      "\tat org.apache.logging.log4j.core.appender.OutputStreamManager.flushBuffer(OutputStreamManager.java:296)\n",
      "\tat org.apache.logging.log4j.core.appender.OutputStreamManager.flush(OutputStreamManager.java:307)\n",
      "\tat org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.directEncodeEvent(AbstractOutputStreamAppender.java:229)\n",
      "\tat org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.tryAppend(AbstractOutputStreamAppender.java:220)\n",
      "\tat org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.append(AbstractOutputStreamAppender.java:211)\n",
      "\tat org.apache.logging.log4j.core.config.AppenderControl.tryCallAppender(AppenderControl.java:160)\n",
      "\tat org.apache.logging.log4j.core.config.AppenderControl.callAppender0(AppenderControl.java:133)\n",
      "\tat org.apache.logging.log4j.core.config.AppenderControl.callAppenderPreventRecursion(AppenderControl.java:124)\n",
      "\tat org.apache.logging.log4j.core.config.AppenderControl.callAppender(AppenderControl.java:88)\n",
      "\tat org.apache.logging.log4j.core.config.LoggerConfig.callAppenders(LoggerConfig.java:714)\n",
      "\tat org.apache.logging.log4j.core.config.LoggerConfig.processLogEvent(LoggerConfig.java:672)\n",
      "\tat org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:648)\n",
      "\tat org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:549)\n",
      "\tat org.apache.logging.log4j.core.config.AwaitCompletionReliabilityStrategy.log(AwaitCompletionReliabilityStrategy.java:67)\n",
      "\tat org.apache.logging.log4j.core.Logger.logMessage(Logger.java:226)\n",
      "\tat org.apache.logging.slf4j.Log4jLogger.log(Log4jLogger.java:374)\n",
      "\tat org.apache.kafka.common.utils.LogContext$LocationAwareKafkaLogger.writeLog(LogContext.java:434)\n",
      "\tat org.apache.kafka.common.utils.LogContext$LocationAwareKafkaLogger.info(LogContext.java:392)\n",
      "\tat org.apache.kafka.clients.FetchSessionHandler.handleResponse(FetchSessionHandler.java:528)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractFetch.handleFetchResponse(AbstractFetch.java:145)\n",
      "\tat org.apache.kafka.clients.consumer.internals.Fetcher$1.onSuccess(Fetcher.java:82)\n",
      "\tat org.apache.kafka.clients.consumer.internals.Fetcher$1.onSuccess(Fetcher.java:78)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:169)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:129)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:617)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:427)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:312)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:251)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1255)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1186)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1159)\n",
      "\tat org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.fetch(KafkaPartitionSplitReader.java:110)\n",
      "\tat org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)\n",
      "\tat org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:193)\n",
      "\tat org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:130)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:349)\n",
      "\tat org.apache.logging.log4j.core.appender.OutputStreamManager.writeToDestination(OutputStreamManager.java:263)\n",
      "\t... 41 more\n",
      "\n",
      "2025-05-10T09:58:59.215415Z Source Data Fetcher for Source: Kafka Source -> Filter, Map, Map -> Sink: Writer -> Sink: Committer (8/8)#0 ERROR Unable to write to stream /opt/anaconda3/envs/flink-env/lib/python3.10/site-packages/pyflink/log/flink-nguyenthiphuongthao-python-Thaos-mac.lan.log for appender FileAppender org.apache.logging.log4j.core.appender.AppenderLoggingException: Error writing to stream /opt/anaconda3/envs/flink-env/lib/python3.10/site-packages/pyflink/log/flink-nguyenthiphuongthao-python-Thaos-mac.lan.log\n",
      "\tat org.apache.logging.log4j.core.appender.OutputStreamManager.writeToDestination(OutputStreamManager.java:265)\n",
      "\tat org.apache.logging.log4j.core.appender.FileManager.writeToDestination(FileManager.java:335)\n",
      "\tat org.apache.logging.log4j.core.appender.OutputStreamManager.flushBuffer(OutputStreamManager.java:296)\n",
      "\tat org.apache.logging.log4j.core.appender.OutputStreamManager.flush(OutputStreamManager.java:307)\n",
      "\tat org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.directEncodeEvent(AbstractOutputStreamAppender.java:229)\n",
      "\tat org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.tryAppend(AbstractOutputStreamAppender.java:220)\n",
      "\tat org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.append(AbstractOutputStreamAppender.java:211)\n",
      "\tat org.apache.logging.log4j.core.config.AppenderControl.tryCallAppender(AppenderControl.java:160)\n",
      "\tat org.apache.logging.log4j.core.config.AppenderControl.callAppender0(AppenderControl.java:133)\n",
      "\tat org.apache.logging.log4j.core.config.AppenderControl.callAppenderPreventRecursion(AppenderControl.java:124)\n",
      "\tat org.apache.logging.log4j.core.config.AppenderControl.callAppender(AppenderControl.java:88)\n",
      "\tat org.apache.logging.log4j.core.config.LoggerConfig.callAppenders(LoggerConfig.java:714)\n",
      "\tat org.apache.logging.log4j.core.config.LoggerConfig.processLogEvent(LoggerConfig.java:672)\n",
      "\tat org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:648)\n",
      "\tat org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:549)\n",
      "\tat org.apache.logging.log4j.core.config.AwaitCompletionReliabilityStrategy.log(AwaitCompletionReliabilityStrategy.java:67)\n",
      "\tat org.apache.logging.log4j.core.Logger.logMessage(Logger.java:226)\n",
      "\tat org.apache.logging.slf4j.Log4jLogger.log(Log4jLogger.java:374)\n",
      "\tat org.apache.kafka.common.utils.LogContext$LocationAwareKafkaLogger.writeLog(LogContext.java:434)\n",
      "\tat org.apache.kafka.common.utils.LogContext$LocationAwareKafkaLogger.info(LogContext.java:392)\n",
      "\tat org.apache.kafka.clients.FetchSessionHandler.handleResponse(FetchSessionHandler.java:528)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractFetch.handleFetchResponse(AbstractFetch.java:145)\n",
      "\tat org.apache.kafka.clients.consumer.internals.Fetcher$1.onSuccess(Fetcher.java:82)\n",
      "\tat org.apache.kafka.clients.consumer.internals.Fetcher$1.onSuccess(Fetcher.java:78)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:169)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:129)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:617)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:427)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:312)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:251)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1255)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1186)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1159)\n",
      "\tat org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.fetch(KafkaPartitionSplitReader.java:110)\n",
      "\tat org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)\n",
      "\tat org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:193)\n",
      "\tat org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:130)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:349)\n",
      "\tat org.apache.logging.log4j.core.appender.OutputStreamManager.writeToDestination(OutputStreamManager.java:263)\n",
      "\t... 41 more\n",
      "\n",
      "2025-05-10T09:58:59.215783Z Source Data Fetcher for Source: Kafka Source -> Filter, Map, Map -> Sink: Writer -> Sink: Committer (8/8)#0 ERROR An exception occurred processing Appender FileAppender org.apache.logging.log4j.core.appender.AppenderLoggingException: Error writing to stream /opt/anaconda3/envs/flink-env/lib/python3.10/site-packages/pyflink/log/flink-nguyenthiphuongthao-python-Thaos-mac.lan.log\n",
      "\tat org.apache.logging.log4j.core.appender.OutputStreamManager.writeToDestination(OutputStreamManager.java:265)\n",
      "\tat org.apache.logging.log4j.core.appender.FileManager.writeToDestination(FileManager.java:335)\n",
      "\tat org.apache.logging.log4j.core.appender.OutputStreamManager.flushBuffer(OutputStreamManager.java:296)\n",
      "\tat org.apache.logging.log4j.core.appender.OutputStreamManager.flush(OutputStreamManager.java:307)\n",
      "\tat org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.directEncodeEvent(AbstractOutputStreamAppender.java:229)\n",
      "\tat org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.tryAppend(AbstractOutputStreamAppender.java:220)\n",
      "\tat org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.append(AbstractOutputStreamAppender.java:211)\n",
      "\tat org.apache.logging.log4j.core.config.AppenderControl.tryCallAppender(AppenderControl.java:160)\n",
      "\tat org.apache.logging.log4j.core.config.AppenderControl.callAppender0(AppenderControl.java:133)\n",
      "\tat org.apache.logging.log4j.core.config.AppenderControl.callAppenderPreventRecursion(AppenderControl.java:124)\n",
      "\tat org.apache.logging.log4j.core.config.AppenderControl.callAppender(AppenderControl.java:88)\n",
      "\tat org.apache.logging.log4j.core.config.LoggerConfig.callAppenders(LoggerConfig.java:714)\n",
      "\tat org.apache.logging.log4j.core.config.LoggerConfig.processLogEvent(LoggerConfig.java:672)\n",
      "\tat org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:648)\n",
      "\tat org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:549)\n",
      "\tat org.apache.logging.log4j.core.config.AwaitCompletionReliabilityStrategy.log(AwaitCompletionReliabilityStrategy.java:67)\n",
      "\tat org.apache.logging.log4j.core.Logger.logMessage(Logger.java:226)\n",
      "\tat org.apache.logging.slf4j.Log4jLogger.log(Log4jLogger.java:374)\n",
      "\tat org.apache.kafka.common.utils.LogContext$LocationAwareKafkaLogger.writeLog(LogContext.java:434)\n",
      "\tat org.apache.kafka.common.utils.LogContext$LocationAwareKafkaLogger.info(LogContext.java:392)\n",
      "\tat org.apache.kafka.clients.FetchSessionHandler.handleResponse(FetchSessionHandler.java:528)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractFetch.handleFetchResponse(AbstractFetch.java:145)\n",
      "\tat org.apache.kafka.clients.consumer.internals.Fetcher$1.onSuccess(Fetcher.java:82)\n",
      "\tat org.apache.kafka.clients.consumer.internals.Fetcher$1.onSuccess(Fetcher.java:78)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:169)\n",
      "\tat org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:129)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:617)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:427)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:312)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:251)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1255)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1186)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1159)\n",
      "\tat org.apache.flink.connector.kafka.source.reader.KafkaPartitionSplitReader.fetch(KafkaPartitionSplitReader.java:110)\n",
      "\tat org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)\n",
      "\tat org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:193)\n",
      "\tat org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:130)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: java.io.IOException: No space left on device\n",
      "\tat java.base/java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.base/java.io.FileOutputStream.write(FileOutputStream.java:349)\n",
      "\tat org.apache.logging.log4j.core.appender.OutputStreamManager.writeToDestination(OutputStreamManager.java:263)\n",
      "\t... 41 more\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m (\n\u001b[1;32m      3\u001b[0m     env\u001b[38;5;241m.\u001b[39mfrom_source(source, WatermarkStrategy\u001b[38;5;241m.\u001b[39mno_watermarks(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKafka Source\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mfilter(check_record_keys)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39msink_to(sink)\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# ==== Thực thi Flink job ====\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflink_datastream_demo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour job has been started successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/flink-env/lib/python3.10/site-packages/pyflink/datastream/stream_execution_environment.py:622\u001b[0m, in \u001b[0;36mStreamExecutionEnvironment.execute\u001b[0;34m(self, job_name)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;124;03mTriggers the program execution. The environment will execute all parts of\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03mthe program that have resulted in a \"sink\" operation. Sink operations are\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;124;03m:return: The result of the job execution, containing elapsed time and accumulators.\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    621\u001b[0m j_stream_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_stream_graph(clear_transformations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, job_name\u001b[38;5;241m=\u001b[39mjob_name)\n\u001b[0;32m--> 622\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m JobExecutionResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_j_stream_execution_environment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mj_stream_graph\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/flink-env/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/flink-env/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/flink-env/lib/python3.10/site-packages/py4j/java_gateway.py:1217\u001b[0m, in \u001b[0;36mGatewayConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m   1214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1217\u001b[0m     answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1218\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mstartswith(proto\u001b[38;5;241m.\u001b[39mRETURN_MESSAGE):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/flink-env/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==== Pipeline xử lý dữ liệu ====\n",
    "(\n",
    "    env.from_source(source, WatermarkStrategy.no_watermarks(), \"Kafka Source\")\n",
    "    .filter(check_record_keys)\n",
    "    .map(print_features, output_type=Types.STRING())\n",
    "    .map(filter_features, output_type=Types.STRING())\n",
    "    .sink_to(sink)\n",
    ")\n",
    "\n",
    "# ==== Thực thi Flink job ====\n",
    "env.execute(\"flink_datastream_demo\")\n",
    "print(\"Your job has been started successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4d8f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flink-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
